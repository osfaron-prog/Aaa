import streamlit as st
import pandas as pd
import io
import json
import re
import spacy
from google.cloud import vision
from google.oauth2 import service_account
from pdf2image import convert_from_bytes
from transformers import pipeline
from PIL import Image, ImageOps

# 1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© ÙˆØ§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø¯Ø§ÙƒÙ† (Quantum Dark UI)
st.set_page_config(page_title="NEURAL DOC HUB", layout="wide", page_icon="ğŸŒ‘")

st.markdown("""
    <style>
    /* Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¯Ø§ÙƒÙ† Ø§Ù„Ø¹Ù…ÙŠÙ‚ */
    .stApp { background-color: #0b0e14; color: #e6edf3; }
    h1, h2, h3 { color: #58a6ff !important; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; }
    
    /* ØªØµÙ…ÙŠÙ… Ø§Ù„ÙƒØ±ÙˆØª Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠØ© */
    .ai-card {
        background: #161b22;
        padding: 20px;
        border-radius: 12px;
        border: 1px solid #30363d;
        margin-bottom: 15px;
        box-shadow: 0 4px 12px rgba(0,0,0,0.3);
    }
    
    /* ØªØ­Ø³ÙŠÙ† ÙˆØ¶ÙˆØ­ Ø§Ù„Ø­Ø±ÙˆÙ */
    .ocr-output {
        font-family: 'Consolas', 'Courier New', monospace;
        color: #d1d5db;
        background: #0d1117;
        padding: 20px;
        border-radius: 10px;
        border: 1px solid #58a6ff;
        line-height: 1.7;
        font-size: 16px;
    }
    
    /* Ø£Ø²Ø±Ø§Ø± Ù…Ø®ØµØµØ© */
    .stButton>button { width: 100%; background-color: #238636; color: white !important; border: none; }
    </style>
    """, unsafe_allow_html=True)

# 2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª (AI Engines)
@st.cache_resource
def load_all_engines():
    # Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙ„Ø®ÙŠØµ (DistilBART)
    summarizer = pipeline("summarization", model="sshleifer/distilbart-cnn-12-6")
    # Ù…Ø­Ø±Ùƒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ ÙˆØ§Ù„Ø¬Ù‡Ø§Øª (Spacy)
    try:
        nlp = spacy.load("en_core_web_sm")
    except:
        import os
        os.system("python -m spacy download en_core_web_sm")
        nlp = spacy.load("en_core_web_sm")
    return summarizer, nlp

# 3. Ø§Ù„Ø±Ø¨Ø· Ù…Ø¹ Google Vision API
def get_vision_client():
    try:
        if "GCP_JSON" in st.secrets:
            json_info = json.loads(st.secrets["GCP_JSON"].strip(), strict=False)
            creds = service_account.Credentials.from_service_account_info(json_info)
            return vision.ImageAnnotatorClient(credentials=creds)
    except Exception as e:
        st.error(f"Credentials Error: {e}")
    return None

# 4. Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© ÙˆØ§Ù„Ù…Ù†Ø·Ù‚ (The App Body)
summarizer, nlp = load_all_engines()
client = get_vision_client()

st.title("ğŸŒ‘ NEURAL DOCUMENT INTELLIGENCE HUB")
st.caption("The ultimate hybrid system for OCR, NLP Analysis, and Risk Assessment")

if client:
    with st.sidebar:
        st.header("âš™ï¸ Control Panel")
        st.info("System: Connected to Google Cloud AI")
        st.divider()
        mode = st.radio("Processing Mode", ["High Accuracy", "Fast Scan"])
        st.write("---")
        st.success("NLP Models: Ready")

    uploaded_file = st.file_uploader("Drop your Document (PDF/Image)", type=["pdf", "jpg", "png", "jpeg"])

    if uploaded_file:
        with st.spinner('ğŸ§¬ Synchronizing Neural Networks...'):
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù ÙˆØªØ­ÙˆÙŠÙ„Ù‡ Ù„Ù†Øµ
            if uploaded_file.type == "application/pdf":
                images = convert_from_bytes(uploaded_file.read())
            else:
                images = [Image.open(uploaded_file)]

            full_text = ""
            for img in images:
                buf = io.BytesIO()
                img.save(buf, format='PNG')
                response = client.document_text_detection(image=vision.Image(content=buf.getvalue()))
                full_text += response.full_text_annotation.text + "\n"

            # 5. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (The AI Logic)
            # Ø£. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª (NER)
            doc = nlp(full_text[:5000])
            entities = [{"Text": ent.text, "Category": ent.label_} for ent in doc.ents]
            
            # Ø¨. ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø®Ø§Ø·Ø± (Risk Check)
            risk_keywords = ["fine", "penalty", "court", "lawsuit", "urgent", "ØºØ±Ø§Ù…Ø©", "Ù…Ø­ÙƒÙ…Ø©", "ÙÙˆØ±ÙŠ"]
            risk_score = sum(15 for word in risk_keywords if word in full_text.lower())
            risk_score = min(risk_score, 100)

            # 6. Ù„ÙˆØ­Ø© Ø§Ù„ØªØ­ÙƒÙ… (The Dashboard)
            col_left, col_right = st.columns([2, 1])

            with col_left:
                st.markdown('<div class="ai-card">', unsafe_allow_html=True)
                st.subheader("ğŸ¤– AI Executive Summary")
                if len(full_text.split()) > 40:
                    summary_text = summarizer(full_text[:1024], max_length=150, min_length=40)[0]['summary_text']
                    st.write(summary_text)
                else:
                    st.write("Insufficient data for neural summarization.")
                st.markdown('</div>', unsafe_allow_html=True)

                st.markdown('<div class="ai-card">', unsafe_allow_html=True)
                st.subheader("ğŸ“Š Structured Intelligence (NER)")
                if entities:
                    st.dataframe(pd.DataFrame(entities).drop_duplicates(), use_container_width=True, height=250)
                else:
                    st.write("No named entities detected.")
                st.markdown('</div>', unsafe_allow_html=True)

            with col_right:
                st.markdown('<div class="ai-card">', unsafe_allow_html=True)
                st.subheader("âš–ï¸ Compliance Score")
                st.metric("Risk Level", f"{risk_score}%", delta="- Negative" if risk_score > 40 else "+ Healthy")
                st.progress(risk_score / 100)
                st.markdown('</div>', unsafe_allow_html=True)

                st.markdown('<div class="ai-card">', unsafe_allow_html=True)
                st.subheader("ğŸ’¬ Smart Q&A")
                q = st.text_input("Ask a question about the doc:")
                if q:
                    if "date" in q.lower() or "ØªØ§Ø±ÙŠØ®" in q:
                        found_date = re.search(r"(\d{1,4}[-/]\d{1,2}[-/]\d{2,4})", full_text)
                        st.info(f"Found: {found_date.group(0)}" if found_date else "No date found.")
                    else:
                        st.write("Contextual searching...")
                st.markdown('</div>', unsafe_allow_html=True)

            # 7. Ù‚Ø³Ù… Ø§Ù„Ø­Ø±ÙˆÙ ÙˆØ§Ù„ÙˆØ¶ÙˆØ­ (The Visible Font Section)
            st.divider()
            tab_view, tab_raw, tab_export = st.tabs(["ğŸ–¼ï¸ Document View", "ğŸ“œ Decoded Stream", "ğŸ“¥ Export Report"])
            
            with tab_view:
                st.image(images[0], use_container_width=True)
            
            with tab_raw:
                st.markdown(f'<div class="ocr-output">{full_text}</div>', unsafe_allow_html=True)

            with tab_export:
                excel_df = pd.DataFrame(entities)
                output = io.BytesIO()
                with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                    excel_df.to_excel(writer, index=False)
                st.download_button("ğŸ“¥ Download Intelligence Report (Excel)", output.getvalue(), "AI_Analysis.xlsx")

else:
    st.error("GCP_JSON MISSING IN SECRETS! Please activate the AI Brain first.")
